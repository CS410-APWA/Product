{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import csv\n",
    "import gensim\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim import corpora, models\n",
    "import xlrd\n",
    "import re\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_essays(essay_path):\n",
    "    \"\"\"Uploads essays from given path and stores them in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        essay_path: A string representing the path to the essays directory.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of filename (string) -> essay corpus (string).\n",
    "\n",
    "    Raises:\n",
    "        Error if path to essay directory is not valid.\n",
    "    \"\"\"\n",
    "\n",
    "    # Should raise error if path not valid: maybe use try/except?\n",
    "    files = os.listdir(essay_path)\n",
    "\n",
    "    essays = {}\n",
    "    for file in files:\n",
    "        # Attempt to confidently guess encoding;\n",
    "        # Otherwise, default to ISO-8859-1.\n",
    "        encoding = \"ISO-8859-1\"\n",
    "        guess = chardet.detect(open(essay_path + file, \"rb\").read())\n",
    "\n",
    "        if (guess[\"confidence\"] >= 0.95):\n",
    "            encoding = guess[\"encoding\"]\n",
    "\n",
    "        with open(essay_path + file, \"r\", encoding=encoding) as f:\n",
    "            essays[file] = f.read()\n",
    "    \n",
    "    return essays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload (from some directory) and store the essays.\n",
    "root = os.path.dirname(os.path.realpath('__file__'))\n",
    "essay_path = root + '/essays/'\n",
    "essays = upload_essays(essay_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    \"\"\"\n",
    "    Converts a given word (string) to its lemmatized version.\n",
    "\n",
    "    Args:\n",
    "        word (string)\n",
    "\n",
    "    Returns:\n",
    "        The lemmatized version of the word (string).\n",
    "    \"\"\"\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts.\n",
    "\n",
    "        Args:\n",
    "            word (string).\n",
    "\n",
    "        Returns:\n",
    "            A part of speech parameter that charactrizes the given word (char).\n",
    "        \"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                    \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                    \"V\": nltk.corpus.wordnet.VERB,\n",
    "                    \"R\": nltk.corpus.wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word, get_wordnet_pos(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_of_topics_and_process_compound_terms(essays, sheet_path):\n",
    "    \"\"\"\n",
    "        - Reads a spreadsheet of topics/defining terms, and builds a Dictionary of\n",
    "    topic -> (defining_term -> score).\n",
    "        - Checks for compound defining terms and processes them in essays.\n",
    "\n",
    "    Args:\n",
    "        essays: Dictionary of filename (string) -> essay (string).\n",
    "\n",
    "    Returns:\n",
    "        A tuple: Dictionary of topics, Dictionary of essays w/ procecessed\n",
    "        compound terms.\n",
    "    \"\"\"\n",
    "\n",
    "    workbook = xlrd.open_workbook(sheet_path)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    topic_term_dict = {}\n",
    "\n",
    "    # Read the first column (TOPIC) and add topics as keys to the dictionary.\n",
    "    current_topic = \"\"\n",
    "    for i in range(1, sheet.nrows):\n",
    "        topic = sheet.cell_value(i, 0)\n",
    "        if topic:\n",
    "            topic_term_dict[topic] = {}\n",
    "            current_topic = topic\n",
    "\n",
    "        term = sheet.cell_value(i, 1)\n",
    "        if term:\n",
    "            # Compound ? If yes, remove spacing.\n",
    "            if ' ' in term:\n",
    "                spacefree_term = ''.join(term.split(' '))\n",
    "\n",
    "                # Replace all occurences of compound terms by removing spaces.\n",
    "                for (label, corpus) in essays.items():\n",
    "                    essays[label] = re.sub(term, spacefree_term, corpus)\n",
    "                \n",
    "                term = spacefree_term\n",
    "\n",
    "            # Lemmatize.\n",
    "            lemmatized_term = lemmatize_word(term)\n",
    "\n",
    "            # Append lemmatized terms.\n",
    "            adjusted_score = 10 - int(sheet.cell_value(i, 2)) + 1\n",
    "\n",
    "            # Store term + score in dictionary (no duplicates).\n",
    "            if lemmatized_term not in topic_term_dict[current_topic]:\n",
    "                topic_term_dict[current_topic][lemmatized_term] = adjusted_score\n",
    "\n",
    "    return topic_term_dict, essays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in spreadsheet of topics/defining terms in order to:\n",
    "#   1. Build a dictionary of topics w/ defining terms + scores.\n",
    "#   2. Preprocess compound defining terms in the essays.\n",
    "spreadsheet_path = \"topic_term_sheet.xlsx\"\n",
    "topic_dict, essays = build_dict_of_topics_and_process_compound_terms(\n",
    "                            essays, spreadsheet_path)\n",
    "\n",
    "#for topic in topic_dict.keys():\n",
    "#    for term in topic_dict[topic].keys():\n",
    "#        print(term)\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_essays(essays):\n",
    "    \"\"\"\n",
    "    Converts each essay from a string to a list of strings (tokens), while\n",
    "    disregarding words that are too short/long.\n",
    "\n",
    "    Args:\n",
    "        essays: A dictionary of filename (string) -> essay corpus (string).\n",
    "\n",
    "    Returns:\n",
    "        A dicionary of filename (string) -> tokenized corpus (list of strings).\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_essays = {}\n",
    "    for (filename, corpus) in essays.items():\n",
    "        tokenized_essays[filename] = gensim.utils.simple_preprocess(\n",
    "            corpus, deacc=True, min_len=2, max_len=20)\n",
    "\n",
    "    return tokenized_essays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize essays.\n",
    "essays = tokenize_essays(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_essays(tokenized_essays):\n",
    "    \"\"\"\n",
    "    Converts the tokens (words) of each essay into lemmatized tokens.\n",
    "\n",
    "    Args:\n",
    "        A dicionary of filename (string) -> tokenized corpus (list of strings).\n",
    "\n",
    "    Returns:\n",
    "        A dicionary of filename (string) -> tokenized+lemmatized corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatized_essays = {}\n",
    "    for (label, word_lst) in tokenized_essays.items():\n",
    "        lemmatized_essays[label] = []\n",
    "        for word in word_lst:\n",
    "            lemmatized_essays[label].append(lemmatize_word(word))\n",
    "\n",
    "    return lemmatized_essays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize all essay tokens.\n",
    "essays = lemmatize_essays(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(lemmatized_essays):\n",
    "    \"\"\"\n",
    "    Removes any tokens charactrized as stop words from the essay tokens.\n",
    "\n",
    "    Args:\n",
    "        A dicionary of filename (string) -> tokenized+lemmatized corpus.\n",
    "\n",
    "    Returns:\n",
    "        A dicionary of filename (string) -> essay corpus w/o stop words.\n",
    "    \"\"\"\n",
    "\n",
    "    english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "    custom_stopwords = open(\"custom_stopwords.txt\", \"r\").read().splitlines()\n",
    "\n",
    "    stopwords_free_essays = {}\n",
    "    for (label, word_lst) in lemmatized_essays.items():\n",
    "        stopwords_free_essays[label] = []\n",
    "        for word in word_lst:\n",
    "            if word not in english_stopwords + custom_stopwords:\n",
    "                stopwords_free_essays[label].append(word)\n",
    "\n",
    "    return stopwords_free_essays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any tokens identified as stop words.\n",
    "essays = remove_stopwords(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_essays(preprocessed_essays):\n",
    "    \"\"\"\n",
    "    Converts each essay into a vector representation using Doc2Vec.\n",
    "\n",
    "    Args:\n",
    "        A dictionary of tokenized + lemmatized + stopwords_free essays.\n",
    "\n",
    "    Returns:\n",
    "        A Dataframe of essays (rows) and vector representation (cols) in\n",
    "        100 dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Vectorize w/ doc2vec.\n",
    "    for i, doc in enumerate(preprocessed_essays.values()):\n",
    "        documents = [TaggedDocument(doc, [i]) ]\n",
    "\n",
    "    d2v_model = Doc2Vec(documents, vector_size=100)\n",
    "    vectorized_df = pd.DataFrame(d2v_model.docvecs.vectors_docs)\n",
    "\n",
    "    # Feature scaling through standardization.\n",
    "    stdsclr = StandardScaler()\n",
    "    standardized_df = pd.DataFrame(\n",
    "            stdsclr.fit_transform(vectorized_df.astype(float)))\n",
    "\n",
    "    return standardized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vector representation of essays.\n",
    "vectorized_essays_df = vectorize_essays(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_with_kmeans(standardized_df, num_of_clusters, preprocessed_essays):\n",
    "    \"\"\"\n",
    "    Partitions essays into clusters using k-means.\n",
    "\n",
    "    Args:\n",
    "        standardized_df: Vector representation of essays (DataFrame).\n",
    "        num_of_clusters: Predetermined number of cluster (int).\n",
    "        preprocessed_essays: Dictionary of essays (filename -> corpus tokens).\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame of essays w/ corresponding cluster number\n",
    "        (row: essays, cols: cluster id, essay corpus, filename).\n",
    "    \"\"\"\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_of_clusters, init=\"k-means++\", max_iter=100)\n",
    "    kmeans.fit(standardized_df.values)\n",
    "\n",
    "    cluster_df = standardized_df\n",
    "    cluster_df['cluster'] = kmeans.labels_\n",
    "    cluster_df['essay'] = preprocessed_essays.values()\n",
    "    cluster_df['filename'] = preprocessed_essays.keys()\n",
    "\n",
    "    return cluster_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster essays using k-means.\n",
    "num_of_clusters = 7     # Should maybe be a global var?\n",
    "\n",
    "essays_with_assigned_cluster_df = cluster_with_kmeans(vectorized_essays_df,\n",
    "                                                      num_of_clusters,\n",
    "                                                      essays)\n",
    "essays_with_assigned_cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_essays_per_cluster(cluster_df, num_of_clusters):\n",
    "    \"\"\"\n",
    "    Gets all essays corpuses within each of the clusters.\n",
    "\n",
    "    Args:\n",
    "        cluster_df : A DataFrame of essays w/ corresponding cluster number.\n",
    "        num_of_clusters : Predetermined number of cluster (int).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of cluster_id (int) -> essays (list of lists of strings).\n",
    "    \"\"\"\n",
    "\n",
    "    essays_per_cluster = {}\n",
    "\n",
    "    for i in range(num_of_clusters):\n",
    "        essays_per_cluster[i] = list(cluster_df[cluster_df.cluster == i].essay)\n",
    "\n",
    "    return essays_per_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames_per_cluster(cluster_df, num_of_clusters):\n",
    "    \"\"\"\n",
    "    Gets all filenames within each of the clusters.\n",
    "\n",
    "    Args:\n",
    "        cluster_df : A DataFrame of essays w/ corresponding cluster number.\n",
    "        num_of_clusters : Predetermined number of cluster (int).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of cluster_id (int) -> filename (string)).\n",
    "    \"\"\"\n",
    "\n",
    "    filenames_per_cluster = {}\n",
    "\n",
    "    for i in range(num_of_clusters):\n",
    "        filenames_per_cluster[i] = list(\n",
    "                                cluster_df[cluster_df.cluster == i].filename)\n",
    "\n",
    "    return filenames_per_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df_with_topic_scores(cluster_df, num_of_clusters, topic_term_dict):\n",
    "    \"\"\"\n",
    "    Updates the Dataframe containing essays w/ clusters, w/ scores corresponding\n",
    "    to the topics.\n",
    "\n",
    "    Args:\n",
    "        cluster_df : A DataFrame of essays w/ corresponding cluster number.\n",
    "        num_of_clusters : Predetermined number of cluster (int).\n",
    "        topic_term_dict : Dictionary of topics w/ defining terms.\n",
    "\n",
    "    Returns:\n",
    "        An updated version of the given dataframe (cluster_df).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add cloumn for each topic, and initialize all essay scores to 0.\n",
    "    for topic in topic_term_dict:\n",
    "        cluster_df[topic] = 0\n",
    " \n",
    "    filenames_per_cluster = get_filenames_per_cluster(cluster_df,\n",
    "                                                      num_of_clusters)\n",
    "    for i in range(num_of_clusters):\n",
    "        for filename in filenames_per_cluster[i]:\n",
    "            essay = list(cluster_df[cluster_df.filename == filename].essay)\n",
    "            dictionary = corpora.Dictionary(essay)\n",
    "            essay_corpus = [dictionary.doc2bow(token) for token in essay]\n",
    "            lda = models.ldamodel.LdaModel(corpus=essay_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=1, passes=10)\n",
    "\n",
    "            # Get\"topic terms\" for each essay using LDA.\n",
    "            essay_term_score = {}\n",
    "            for idx, terms in lda.print_topics(0, 100):\n",
    "                \n",
    "                # LDA generates topic terms in the format: \"term1*score1 + term2*score2 + ...\"\".\n",
    "                for term_with_score in terms.split('+'):\n",
    "                    \n",
    "                    # Separate terms/scores from LDA generated string.\n",
    "                    term = term_with_score.split('*')[1][1:-2]\n",
    "                    score = term_with_score.split('*')[0]\n",
    "\n",
    "                    # Build a dictionary of all the topic terms of the essay w/ corresponding scores.\n",
    "                    essay_term_score[term] = float(score)\n",
    "\n",
    "            # For each topic term extracted for an essay, check if it's in the topic dictionary.\n",
    "            essay_topic_term_score = {}\n",
    "            for term in essay_term_score.keys():\n",
    "                for topic in topic_term_dict.keys():\n",
    "                    if term in topic_term_dict[topic].keys():\n",
    "                        # If a term is found in the the topic dictionary, compute it's score and update its value.\n",
    "                        score = essay_term_score[term] * topic_term_dict[topic][term]\n",
    "                        if topic in essay_term_score:\n",
    "                            essay_topic_term_score[topic] += score\n",
    "                        else:\n",
    "                            essay_topic_term_score[topic] = score\n",
    "            \n",
    "            # For each essay, add a score corresponding to a topic that corresponds to it.\n",
    "            for topic, score in essay_topic_term_score.items():\n",
    "                cluster_df.loc[cluster_df[cluster_df['filename'] == filename].index, topic] = score\n",
    "\n",
    "    return cluster_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each essay, assign an initial score to each of its relevent topics.\n",
    "essay_with_topic_scores_df = update_df_with_topic_scores(\n",
    "                                essays_with_assigned_cluster_df,\n",
    "                                num_of_clusters, topic_dict)\n",
    "\n",
    "essay_with_topic_scores_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_essay_rank(cluster_df, topic_term_dict, num_clusters):\n",
    "    \"\"\"\n",
    "    Ranks the essays by ... (Matt?)\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    def distance(v1, v2):\n",
    "        # L2 norm\n",
    "        return np.linalg.norm(v1-v2)\n",
    "\n",
    "    # calculate global centroids for each topic\n",
    "    topic_globcentroid = {}\n",
    "\n",
    "    for topic in topic_term_dict.keys():\n",
    "        # get all vector columns matching topic\n",
    "        sub_df = cluster_df[cluster_df[topic] != 0].iloc[:, :100]\n",
    "\n",
    "        # number of vectors with that topic\n",
    "        n = len(sub_df)\n",
    "\n",
    "        # mean of all vectors is centroid\n",
    "        globalcentroid = sum([sub_df.iloc[i] for i in range(n)])/n\n",
    "        topic_globcentroid[topic] = globalcentroid\n",
    "\n",
    "    # calculate local centroids for each topic\n",
    "    for cluster in range(num_clusters):\n",
    "        for topic in topic_term_dict.keys():\n",
    "            # get all vector columns matching cluster and topic\n",
    "            sub_df = cluster_df[cluster_df['cluster'] == cluster]\n",
    "            sub_df = sub_df[sub_df[topic] != 0].iloc[:, :100]\n",
    "\n",
    "            # number of vectors in cluster with this topic\n",
    "            n = len(sub_df)\n",
    "\n",
    "            # mean of all vectors is centroid with this topic\n",
    "            localcentroid = sum([sub_df.iloc[i] for i in range(n)])/n\n",
    "\n",
    "            # find distance between current localcentroid and its corresponding\n",
    "            # globalcentoid by topic\n",
    "            d1 = distance(topic_globcentroid[topic], localcentroid)\n",
    "\n",
    "            # find distance between each vector and its corresponding\n",
    "            # localcentroid, update rank\n",
    "            vectors = cluster_df[cluster_df['cluster'] == cluster]\n",
    "            vectors = vectors[vectors[topic] != 0].index\n",
    "            for ident in vectors:\n",
    "                loc = cluster_df.iloc[ident, :100]\n",
    "                d2 = distance(loc, localcentroid)\n",
    "\n",
    "                # update rank - lda score * dist from v to localcentroid * dist\n",
    "                # from localcentroid to globalcentroid\n",
    "                cluster_df.at[ident, topic] = cluster_df.at[ident, topic] * d1 * d2\n",
    "                \n",
    "    all_we_need = cluster_df.iloc[:, 102:]\n",
    "\n",
    "    return all_we_need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the initially assigned scores.\n",
    "all_we_need = update_essay_rank(essay_with_topic_scores_df, topic_dict, num_of_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final df.\n",
    "all_we_need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
